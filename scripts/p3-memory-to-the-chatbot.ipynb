{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part3: Adding Memory to the chatbot\n",
    "When the chatbot can't remember the context of previous interactios, this limits its ability to have coherent, multi-turn conversations.\n",
    "Langgraph solves this problem through **persistent checkpointer**. If you provide a checkpointer when compiling the graph and a thread_id when callin your graph, LangGraph automatically sves the state after each step. When you invoke the graph again using the same thread_id, the graph loads its saved state, allowing the chatbot to pick up where it left off.\n",
    "\n",
    "We will see later that checkpointing is much more powerful than simple chat memory - it lets you save and resume complex state at any time for error recovery, human-in-the-loop workflows, time travel interactions, and more. But before we get too ahead of ourselves, let's add checkpointing to enable multi-turn conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We replace BasicToolNode with LnagGraph's prebuilt ToolNode and tools_condition\n",
    "# because these do some nice things like parallel API execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain.utilities.tavily_search import TavilySearchAPIWrapper\n",
    "from configuracion import TAVILY_API_KEY, MODEL_GEMINI, GCP_PROJECT_ID, GCP_REGION, scoped_creds\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "tavilySearchAPIWrapper = TavilySearchAPIWrapper(tavily_api_key=TAVILY_API_KEY)\n",
    "tool = TavilySearchResults(max_results=2, api_wrapper=tavilySearchAPIWrapper)\n",
    "tools = [tool]\n",
    "llm = ChatVertexAI(\n",
    "                model_name=MODEL_GEMINI,\n",
    "                project=GCP_PROJECT_ID,\n",
    "                location=GCP_REGION,\n",
    "                credentials=scoped_creds,\n",
    "                max_output_tokens=8192,\n",
    "                temperature=0.2,\n",
    "        )\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "tool_node = ToolNode(tools=[tool])\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    tools_condition\n",
    ")\n",
    "#Any time a tool is called, we return to the chatbot to decide the next step\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "\n",
    "#Finally, compile the graph with the provided checkpointer\n",
    "graph = graph_builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "busca en internet noticias sobre mexico?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Lo siento, no tengo acceso a internet para buscar noticias en tiempo real.  Para obtener información actualizada sobre México, te recomiendo visitar sitios web de noticias confiables como:\n",
      "\n",
      "* **El Universal:** https://www.eluniversal.com.mx/\n",
      "* **Milenio:** https://www.milenio.com/\n",
      "* **La Jornada:** https://www.jornada.com.mx/\n",
      "* **CNN en Español:** https://cnnespanol.cnn.com/\n",
      "\n",
      "¡Espero que esto te ayude!\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "user_input = \"busca en internet noticias sobre mexico?\"\n",
    "\n",
    "# The config is the **second positional argument** to stream() or invoke()!\n",
    "events = graph.stream(\n",
    "    {\"messages\": [(\"user\", user_input)]}, config, stream_mode=\"values\"\n",
    ")\n",
    "for event in events:\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langg-int",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
